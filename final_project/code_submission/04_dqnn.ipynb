{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import pandas as pd \n",
    "import numpy  as np\n",
    "import util \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network sizing:\n",
    "NUM_INPUTS       = 144 + 1  # num. network inputs.\n",
    "NUM_OUTPUTS      = 2        # num. network outputs (1 per state, i.e. bet home/away).\n",
    "N_ODDS_PER_TEAM  = 72       # num. odds figures per team. \n",
    "\n",
    "# hyperparam grid search ranges, substitute values as necessary: \n",
    "# eta_options              = [1e-2, 1e-3, 1e-4]\n",
    "# epsilon_options          = [0.3, 0.5, 0.7]\n",
    "# num_exps_to_copy_options = [10000, 20000, 50000]\n",
    "# discount_factor_options  = [0.95, 0.99]\n",
    "\n",
    "# hyperparameters: \n",
    "eta              = 1e-3     # optimizer learning rate.\n",
    "epsilon          = 0.5\n",
    "num_exps_to_copy = 20000    # num. exps. per train/target network sync.\n",
    "\n",
    "# fixed vals:\n",
    "discount_factor  = 0.99     # discount factor for rl update.\n",
    "max_train_eps    = 1000     # max. number of training episodes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to represent the deep, fully-connected neural network: \n",
    "class DeepQNN(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # instantiate the model, which consists of a three layer \n",
    "        # (input-hidden-output) structure. \n",
    "        self.ll1 = nn.Linear(NUM_INPUTS, NUM_INPUTS)\n",
    "        self.ll2 = nn.Linear(NUM_INPUTS, NUM_OUTPUTS)\n",
    "        \n",
    "        # initialize layers: \n",
    "        nn.init.xavier_uniform_(self.ll1.weight)\n",
    "        nn.init.xavier_uniform_(self.ll2.weight)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x1  = F.relu(self.ll1(state))\n",
    "        out = self.ll2(x1)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data:\n",
    "x_train, y_train = util.load_dataset('../data/xy_train_data.csv')\n",
    "\n",
    "# state generator function: \n",
    "def get_curr_and_next_state():\n",
    "    # track idx: \n",
    "    idx = 0\n",
    "    while idx < len(x_train):\n",
    "        # get current state and final winner odds: \n",
    "        curr_state, curr_lbl = x_train[idx], y_train[idx]\n",
    "        final_winner_odds = curr_state[-1] if curr_lbl == 1 else curr_state[N_ODDS_PER_TEAM - 1]\n",
    "        # get some random next state, it doesn't matter: \n",
    "        rd.seed(229)\n",
    "        next_state_idx = rd.randint(0, len(x_train) - 1)\n",
    "        next_state, next_lbl = x_train[next_state_idx], y_train[next_state_idx]\n",
    "        # yield result: \n",
    "        yield (curr_state, next_state, curr_lbl, next_lbl, final_winner_odds)\n",
    "        # increment the index: \n",
    "        idx += 1  \n",
    "    \n",
    "# ex. of how to use the generator: \n",
    "# state_gen = get_curr_and_next_state()\n",
    "# curr_state, next_state, curr_lbl, next_lbl, final_winner_odds = next(state_gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING LOOP SET-UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take over any available gpus: \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# init. the networks. convert to float. \n",
    "train_net, target_net = DeepQNN().to(device).float(), DeepQNN().to(device).float()\n",
    "\n",
    "# init. optimizer: \n",
    "optimizer = torch.optim.SGD(train_net.parameters(), lr=eta)\n",
    "\n",
    "# init. loss fn: \n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# track total experiences: \n",
    "num_exps = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure CUDA is available: \n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy epsilon var: \n",
    "e = epsilon\n",
    "\n",
    "# main training loop: \n",
    "for ep_idx in range(max_train_eps):\n",
    "    \n",
    "    # refresh loss log, balance, state generator:  \n",
    "    loss_log = []\n",
    "    balance = 6000     \n",
    "    state_gen = get_curr_and_next_state()\n",
    "    \n",
    "    # episode concludes when balance <= 0 or all training states visited: \n",
    "    while (balance > 0):\n",
    "        \n",
    "        # acquire next state: \n",
    "        curr_state, next_state, curr_lbl, next_lbl, final_winner_odds = next(state_gen, (None, None, None, None, None))\n",
    "        \n",
    "        # only proceed with training if next training state is\n",
    "        # non-visited (i.e., != None): \n",
    "        if curr_state is not None: \n",
    "            \n",
    "            # STATE: concatenate the balance to `curr_state` to make the \n",
    "            # complete state vector and convert to tensor:\n",
    "            curr_state = np.append(curr_state, [balance])\n",
    "            curr_state = torch.from_numpy(curr_state).to(device)\n",
    "            \n",
    "            # ACTION: generate action (i.e., team to bet on and assoc. \n",
    "            # pred. prob from 0 to 1) for this state using `target_net`:\n",
    "            \n",
    "            # first compute q-val of each action (i.e., logits for betting\n",
    "            # home or away, output from sigmoid): \n",
    "            q_vals = train_net(curr_state.float()).cpu().data.numpy()\n",
    "            \n",
    "            # normalize q-vals to produce prediction probabilities: \n",
    "            tot = sum(q_vals)\n",
    "            pred_probs = [q_val / tot for q_val in q_vals]\n",
    "            \n",
    "            # identify the bet-upon team: \n",
    "            pred_team = np.argmax(pred_probs)\n",
    "            \n",
    "            # apply epsilon-greedy: \n",
    "            epsilon_prob = np.random.uniform()\n",
    "            if epsilon_prob < e: \n",
    "                pred_team = rd.randint(0, 1)\n",
    "            \n",
    "            # extract associated `pred_prob``:\n",
    "            pred_prob = pred_probs[pred_team]\n",
    "        \n",
    "            # REWARD: compute reward (i.e., winnings/losings) depending on \n",
    "            # `pred_prob` and whether `pred_team` == `curr_lbl`:   \n",
    "            reward = 0\n",
    "            pred_multiplier = (pred_prob - 0.5) / 0.5 if pred_prob >= 0.5 else -1 * (pred_prob - 0.5) / 0.5\n",
    "            \n",
    "            if pred_team == curr_lbl: \n",
    "                winnings = final_winner_odds * pred_multiplier if final_winner_odds >= 0 else 100 * pred_multiplier\n",
    "                reward   = winnings\n",
    "                balance += winnings  \n",
    "            else: \n",
    "                losings  = 100 * pred_multiplier if final_winner_odds >= 0 else abs(final_winner_odds) * pred_multiplier\n",
    "                reward   = -1 * losings\n",
    "                balance -= losings\n",
    "            \n",
    "            # NEXT STATE: concatenate the updated balance to the next\n",
    "            # state array and convert to tensor: \n",
    "            next_state = np.append(next_state, [balance])\n",
    "            next_state = torch.from_numpy(next_state).to(device)\n",
    "            \n",
    "            # STATE TERMINAL: state is terminal if balance <= 0:\n",
    "            terminal = 1 if balance <= 0 else 0 \n",
    "            \n",
    "            # UPDATE NETWORK WEIGHTS: determine max q-value assoc. with\n",
    "            # next state and use as target value for q-update. \n",
    "            \n",
    "            # re-compute q-vals for current episode: \n",
    "            q_vals = train_net(curr_state.float())\n",
    "            \n",
    "            # compute the maximum q-vals for next state: \n",
    "            max_sp_q_val = target_net(next_state.float()).max(-1).values \n",
    "            \n",
    "            # compute the q-update target value:\n",
    "            targets = reward + (1 - terminal) * discount_factor * max_sp_q_val\n",
    "            # something else tried: targets = rewards only\n",
    "            # targets = torch.from_numpy(np.asarray([reward])).float().to(device)\n",
    "            \n",
    "            # update the q-values to only retain those corresponding to \n",
    "            # the executed action (i.e., which team was bet upon): \n",
    "            action_mask = F.one_hot(torch.tensor(np.array([pred_team], dtype='int64'), device=device), num_classes=2)\n",
    "            # apply mask and collapse tensor for scalar comps: \n",
    "            q_vals = (action_mask * q_vals).sum(-1) \n",
    "             \n",
    "            # compute loss and backprop! \n",
    "            loss = loss_fn(q_vals, targets.detach().view(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # clip gradients to prevent exploding loss:\n",
    "            torch.nn.utils.clip_grad_norm_(train_net.parameters(), 1.)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # record loss: \n",
    "            loss_log.append(loss)\n",
    "            \n",
    "            # update experience count: \n",
    "            num_exps += 1 \n",
    "            \n",
    "            # copy training network weights to target network if \n",
    "            # `num_exps_to_copy` reached: \n",
    "            if num_exps % num_exps_to_copy == 0: \n",
    "                print('syncing network weights...')\n",
    "                target_net.load_state_dict(train_net.state_dict())\n",
    "        \n",
    "        # if all training states visited, break current loop:  \n",
    "        else:     \n",
    "            break \n",
    "    \n",
    "    # report episode count, final balance, avg. loss:\n",
    "    # if ep_idx % 50 == 0:\n",
    "    print(f'episode {ep_idx}: ending balance :: {balance:.2f}, average exp. loss :: {sum(loss_log) / len(loss_log):.2e}')\n",
    "    \n",
    "    # decay epsilon before next training episode if it is still above \n",
    "    # some threshold: \n",
    "    if e > 0.05:\n",
    "        e -= -0.001\n",
    "            \n",
    "# save model parameters: \n",
    "print('saving model parameters...')\n",
    "torch.save(target_net.state_dict(), './trained_dqnn.params')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALIDATION/TEST SET EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from file if necessary:\n",
    "trained_model = DeepQNN().to(device=device)\n",
    "trained_model.load_state_dict(torch.load('./trained_dqnn.params'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment/uncomment following lines as necessary if using \n",
    "# validation or test set. \n",
    "# x_test, y_test = util.load_dataset('../data/xy_valid_data.csv', True)\n",
    "x_test, y_test = util.load_dataset('../data/xy_test_data.csv', True)\n",
    "\n",
    "balance = 1000\n",
    "\n",
    "preds = []\n",
    "picks = []\n",
    "\n",
    "for entry_idx in range(len(x_test)):\n",
    "    # extract entry:\n",
    "    curr_state, curr_lbl = x_test[entry_idx], y_test[entry_idx]\n",
    "    final_winner_odds = curr_state[-1] if curr_lbl == 1 else curr_state[N_ODDS_PER_TEAM - 1]\n",
    "    \n",
    "    # compute q-vals & normalize for probs:\n",
    "    curr_state = np.append(curr_state, [balance])\n",
    "    curr_state = torch.from_numpy(curr_state).to(device)\n",
    "    \n",
    "    q_vals = trained_model(curr_state.float()).cpu().data.numpy()\n",
    "    \n",
    "    tot = sum(q_vals)\n",
    "    pred_probs = [q_val / tot for q_val in q_vals]\n",
    "    \n",
    "    # identify the bet-upon team: \n",
    "    pred_team = np.argmax(pred_probs)\n",
    "        \n",
    "    # extract associated `pred_prob``:\n",
    "    pred_prob = pred_probs[pred_team]\n",
    "\n",
    "    # REWARD: compute reward (i.e., winnings/losings) depending on \n",
    "    # `pred_prob` and whether `pred_team` == `curr_lbl`:   \n",
    "    reward = 0\n",
    "    pred_multiplier = (pred_prob - 0.5) / 0.5 if pred_prob >= 0.5 else -1 * (pred_prob - 0.5) / 0.5\n",
    "    \n",
    "    if pred_team == curr_lbl: \n",
    "        winnings = final_winner_odds * pred_multiplier if final_winner_odds >= 0 else 100 * pred_multiplier\n",
    "        balance += winnings  \n",
    "    else: \n",
    "        losings  = 100 * pred_multiplier if final_winner_odds >= 0 else abs(final_winner_odds) * pred_multiplier\n",
    "        balance -= losings\n",
    "    \n",
    "    preds.append(pred_prob)\n",
    "    picks.append(pred_team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert picks from 0/1 to 'home'/'away':\n",
    "picks = ['away' if pick == 1. else 'home' for pick in picks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-map probs. if pick is home to maintain prob. multiplier \n",
    "# computation code as defined for logreg model:\n",
    "for idx, (pred, pick) in enumerate(zip(preds, picks)): \n",
    "    if pick == 'home': \n",
    "        preds[idx] = (preds[idx] - 1) / -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test_data.csv')\n",
    "\n",
    "# add `probs` column to dataframe:\n",
    "test['preds'] = preds\n",
    "\n",
    "# add the picks column to the dataframe:\n",
    "test['strategy_picks'] = picks\n",
    "\n",
    "# data preview:\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any unnecessary columns: \n",
    "test = test[['home_avg_odds_hr_71', 'away_avg_odds_hr_71', 'winner', 'favorite', 'has_favorite_for_winner', 'closely_contested', 'strategy_picks', 'preds']]\n",
    "\n",
    "# create a winner_avg_odds column: \n",
    "test['winner_avg_odds_hr_71'] = np.where(\n",
    "    test['winner'] == 'home', test['home_avg_odds_hr_71'], test['away_avg_odds_hr_71']\n",
    ")\n",
    "\n",
    "# create 'relative_underdog' column: \n",
    "test['relative_underdog'] = np.where(\n",
    "    test['favorite'] == 'home', 'away', 'home'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PROFITABLE_BETS = 0\n",
    "N_TOTAL_BETS      = 0\n",
    "\n",
    "NET_VALUE_OF_BETS_WON  = []\n",
    "NET_VALUE_OF_BETS_LOST = []\n",
    "\n",
    "UNDERDOG_BETS_HITS  = []\n",
    "N_UNDERDOG_BETS_TOTAL = 0\n",
    "\n",
    "CLEAR_WINNER_BETS_HITS = []\n",
    "N_CLEAR_WINNER_BETS_TOTAL = 0\n",
    "\n",
    "CLOSELY_CONTESTED_BETS_HITS = []\n",
    "N_CLOSELY_CONTESTED_BETS_TOTAL = 0\n",
    "\n",
    "balance = 1000\n",
    "\n",
    "# iterate through dataframe: \n",
    "for row in test.itertuples(index=False):\n",
    "    \n",
    "    # if there is balance remaining: \n",
    "    if balance >= 0:\n",
    "    \n",
    "        # extract elements: \n",
    "        final_home_odds, final_away_odds, winner, favorite, has_favorite_for_winner, \\\n",
    "            closely_contested, strategy_pick, pred_prob, final_winner_odds, relative_underdog = row\n",
    "            \n",
    "         # update counts:\n",
    "        if has_favorite_for_winner: N_CLEAR_WINNER_BETS_TOTAL += 1\n",
    "        if closely_contested: N_CLOSELY_CONTESTED_BETS_TOTAL += 1\n",
    "        \n",
    "        # compute `pred` multiplier:\n",
    "        # if betting on away (pred = [0.5, 1.0]), mulitplier = (pred - 0.5) / 0.5\n",
    "        # if betting on home (pred = [0.0, 0.5)), multiplier = -1 * (pred - 0.5) / 0.5\n",
    "        if pred_prob >= 0.5:\n",
    "            pred_multiplier = (pred_prob - 0.5) / 0.5\n",
    "        else: \n",
    "            pred_multiplier = -1 * (pred_prob - 0.5) / 0.5\n",
    "        \n",
    "        # if strategy bet on underdog: \n",
    "        if strategy_pick == relative_underdog: \n",
    "            N_UNDERDOG_BETS_TOTAL += 1\n",
    "        \n",
    "        # if pick was correct: \n",
    "        if winner == strategy_pick:\n",
    "            N_PROFITABLE_BETS += 1\n",
    "            \n",
    "            if final_winner_odds >= 0: \n",
    "                NET_VALUE_OF_BETS_WON.append(final_winner_odds * pred_multiplier)\n",
    "            if final_winner_odds < 0:\n",
    "                NET_VALUE_OF_BETS_WON.append(100 * pred_multiplier)\n",
    "            \n",
    "            # if strategy correctly bet on underdog:\n",
    "            if strategy_pick == relative_underdog:\n",
    "                if final_winner_odds >= 0: \n",
    "                    UNDERDOG_BETS_HITS.append(final_winner_odds * pred_multiplier)\n",
    "                if final_winner_odds < 0:\n",
    "                    UNDERDOG_BETS_HITS.append(100 * pred_multiplier)\n",
    "                    \n",
    "            # if the strategy correctly bet in a 'clear winner' match:\n",
    "            if has_favorite_for_winner:\n",
    "                if final_winner_odds >= 0: \n",
    "                    CLEAR_WINNER_BETS_HITS.append(final_winner_odds * pred_multiplier)\n",
    "                if final_winner_odds < 0:\n",
    "                    CLEAR_WINNER_BETS_HITS.append(100 * pred_multiplier)\n",
    "                    \n",
    "            # if the strategy correctly bet in a 'closely contested' match:\n",
    "            if closely_contested: \n",
    "                if final_winner_odds >= 0: \n",
    "                    CLOSELY_CONTESTED_BETS_HITS.append(final_winner_odds * pred_multiplier)\n",
    "                if final_winner_odds < 0:\n",
    "                    CLOSELY_CONTESTED_BETS_HITS.append(100 * pred_multiplier) \n",
    "                    \n",
    "            # update balance: \n",
    "            if final_winner_odds >= 0: \n",
    "                balance += final_winner_odds * pred_multiplier\n",
    "            if final_winner_odds < 0:\n",
    "                balance += 100  * pred_multiplier\n",
    "                                \n",
    "        # if the pick was incorrect:\n",
    "        elif winner != strategy_pick:\n",
    "            if final_winner_odds >= 0: \n",
    "                NET_VALUE_OF_BETS_LOST.append(100 * pred_multiplier)\n",
    "            if final_winner_odds < 0:\n",
    "                NET_VALUE_OF_BETS_LOST.append(abs(final_winner_odds) * pred_multiplier)\n",
    "                \n",
    "            # update balance: \n",
    "            if final_winner_odds >= 0: \n",
    "                balance -= 100 * pred_multiplier\n",
    "            if final_winner_odds < 0:\n",
    "                balance -= abs(final_winner_odds) * pred_multiplier\n",
    "                \n",
    "        # update total bet count:\n",
    "        N_TOTAL_BETS += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profitable_bet_rate       = N_PROFITABLE_BETS / N_TOTAL_BETS\n",
    "overall_profit            = sum(NET_VALUE_OF_BETS_WON) - sum(NET_VALUE_OF_BETS_LOST)\n",
    "largest_amt_won           = max(NET_VALUE_OF_BETS_WON)\n",
    "smallest_amt_won          = min(NET_VALUE_OF_BETS_WON)\n",
    "avg_amt_won               = sum(NET_VALUE_OF_BETS_WON) / len(NET_VALUE_OF_BETS_WON)\n",
    "avg_amt_loss              = sum(NET_VALUE_OF_BETS_LOST) / len(NET_VALUE_OF_BETS_LOST)\n",
    "\n",
    "underdog_bet_hr           = len(UNDERDOG_BETS_HITS) / N_UNDERDOG_BETS_TOTAL  if (N_UNDERDOG_BETS_TOTAL > 0) else 'no underdog bets made.'\n",
    "avg_underdog_hit_winnings = sum(UNDERDOG_BETS_HITS) / len(UNDERDOG_BETS_HITS) if len(UNDERDOG_BETS_HITS) > 0 else 'no winning underdog bets made.'\n",
    "\n",
    "clearwinner_bet_hr           = len(CLEAR_WINNER_BETS_HITS) / N_CLEAR_WINNER_BETS_TOTAL  if (N_CLEAR_WINNER_BETS_TOTAL > 0) else 'no clear favorite bets made.'\n",
    "avg_clearwinner_hit_winnings = sum(CLEAR_WINNER_BETS_HITS) / len(CLEAR_WINNER_BETS_HITS) if len(CLEAR_WINNER_BETS_HITS) > 0 else 'no winning clear favorite bets made.'\n",
    "\n",
    "closelycontested_bet_hr           = len(CLOSELY_CONTESTED_BETS_HITS) / N_CLOSELY_CONTESTED_BETS_TOTAL  if (N_CLOSELY_CONTESTED_BETS_TOTAL > 0) else 'no closely contested bets made.'\n",
    "avg_closelycontested_hit_winnings = sum(CLOSELY_CONTESTED_BETS_HITS) / len(CLOSELY_CONTESTED_BETS_HITS) if len(CLOSELY_CONTESTED_BETS_HITS) > 0 else 'no winning closely contested bets made.'\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    'profitable_bet_rate' : f'{profitable_bet_rate:.2f} ({N_PROFITABLE_BETS} / {N_TOTAL_BETS})',\n",
    "    'overall_profit' : overall_profit,\n",
    "    'largest_amt_won' : largest_amt_won,\n",
    "    'smallest_amt_won' : smallest_amt_won,\n",
    "    'avg_amt_won' : avg_amt_won,\n",
    "    'avg_amt_loss' : avg_amt_loss,\n",
    "    'underdog_bet_hr' : underdog_bet_hr, \n",
    "    'avg_underdog_hit_winnings' : avg_underdog_hit_winnings, \n",
    "    'clearwinner_bet_hr' : clearwinner_bet_hr,\n",
    "    'avg_clearwinner_hit_winnings' : avg_clearwinner_hit_winnings,\n",
    "    'closelycontested_bet_hr' : closelycontested_bet_hr,\n",
    "    'avg_closelycontested_hit_winnings' : avg_closelycontested_hit_winnings\n",
    "}\n",
    "\n",
    "# report metrics\n",
    "for metric in metrics.keys():    \n",
    "    if type(metrics[metric]) is str: \n",
    "        print(f'{metric} :: {metrics[metric]}')\n",
    "    else: \n",
    "        print(f'{metric} :: {metrics[metric]:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
