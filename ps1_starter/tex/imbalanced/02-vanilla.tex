\item \subquestionpoints{5} \textbf{Coding problem: vanilla logistic regression}

First, we use the vanilla logistic regression to learn an imbalanced dataset. For the rest of the question, we will use the dataset and starter code provided in
the following files:
%
\begin{center}
	\begin{itemize}
		\item	\url{src/imbalanced/{train,validation}.csv}
		\item   \url{src/imbalanced/imbalanced.py}
	\end{itemize}
\end{center}


Each file contains $n$ examples, one example $(x^{(i)}, y^{(i)})$ per row. $x$ is two-dimensional, i.e., the $i$-th row contains columns $x^{(i)}_1\in\Re$,
$x^{(i)}_2\in\Re$, and $y^{(i)}\in\{0, 1\}$. Let $\calD=\{(x^{(i)}, y^{(i)})\}_{i=1}^n$ be our training dataset. $\calD$ has $\rho n$ examples with label 1 and $(1-\rho)n$ with label 0. In the dataset we constructed, $\rho=1/11$.

You will train a linear classifier $h_{\theta}(x)$ with average empirical loss for logistical regression, where $h_\theta(x)=g(\theta^T x), g(z)=1/(1+e^{-z})$:
\begin{align*}
J(\theta) = -\frac{1}{\nexp} \sum_{i=1}^\nexp \left(y^{(i)}\log(h_{\theta}(x^{(i)}))
+  (1 - y^{(i)})\log(1 - h_{\theta}(x^{(i)}))\right), 
\end{align*}

You can use the provided logistic regression implementation, or any standard logistic regression library to optimize the objective above. After obtaining the classifier, 
compute the classifier's accuracy ($A$), balanced accuracy ($\overline{A}$), accuracies for the two classes ($A_0, A_1$) on the validation dataset, and report them in the writeup. You are expected to observe that the minority class (positive class) has significantly lower accuracy than the majority class. 


Create a plot to visualize the validation set with $x_1$ on the horizontal axis and $x_2$ on
the vertical axis. Use different symbols for examples $x^{(i)}$ with true label $y^{(i)} = 1$
than those with $y^{(i)} = 0$. On the same figure, plot the decision boundary obtained
by your model (i.e, line corresponding to model's predicted probability = 0.5) in red color. Include
this plot in your writeup.
