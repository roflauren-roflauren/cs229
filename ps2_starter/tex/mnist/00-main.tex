\item \points{30} {\bf Neural Networks: MNIST image classification}

{\bf Note:} This question may requires knowledge on backpropagation covered on Monday of Week 5.

In this problem, you will implement a simple neural network
to classify grayscale images of handwritten digits (0 - 9) from
the MNIST dataset. The dataset contains 60,000 training images and
10,000 testing images of handwritten digits, 0 - 9. Each image is
28$\times$28 pixels in size, and is generally represented as a flat
vector of 784 numbers. It also includes labels for each example, a number
indicating the actual digit (0 - 9) handwritten in that image. A sample of
a few such images are shown below.

\begin{center}
\includegraphics[scale=0.5]{mnist/mnist_plot}
\end{center}


The data and starter code for this problem can be found in

\begin{itemize}
\item \texttt{src/mnist/nn.py}
\item \texttt{src/mnist/images\_train.csv}
\item \texttt{src/mnist/labels\_train.csv}
\item \texttt{src/mnist/images\_test.csv}
\item \texttt{src/mnist/labels\_test.csv}
\end{itemize}

The starter code splits the set
of 60,000 training images and labels into a set of 50,000 examples as
the training set, and 10,000 examples for dev set.

To start, you will implement a neural network with a single hidden layer
and cross entropy loss, and train it with the provided data set. You will use the
sigmoid function as activation for the hidden layer and use the cross-entropy loss for multi-class classification. Recall that for a single example $(x, y)$, the cross
entropy loss is:
%\tnote{change the notation here to make it more consistent with the lecture }
$$\ell_\textup{CE}(\bar{h}_{\theta}(x),y) = - \log\left(\frac{\exp(\bar{h}_{\theta}(x)_{y})}{\sum_{s=1}^{k}\exp({\bar{h}_{\theta}(x)}_s)}\right),$$
where $\bar{h}_{\theta}(x) \in \mathbb{R}^{k}$ is the logits, i.e., the output of the the model on a training example $x$, $\bar{h}_\theta(x)_y$ is the $y$-th coordinate of the vector $\bar{h}_\theta(x)$ (recall that $y\in \{1,\dots, k\}$ and thus can serve as an index.) %\tnote{edited, the old version seems to be wrong}

%\tnote{let's call the one-hot vector $e_y$ so that we don't have to overload the notation. Can still keep the $\hat{y}$. The cross-entropy loss should be defined the same as in the lecture notes}


For clarity, we provide the forward propagation equations below for the neural network with a single hidden layer. We have labeled data $(x^{(i)}, y^{(i)})_{i=1}^n$, where $x^{(i)} \in \mathbb{R}^d$, and $y^{(i)} \in \{1,\dots, k\}$ is ground truth label. Let $m$ be the number of hidden units in the neural network, so that weight matrices $W^{[1]} \in \mathbb{R}^{d \times m}$ and $W^{[2]} \in \mathbb{R}^{m \times k}$.\footnote{Please note that the dimension of the weight matrices is different from those in the lecture notes, but we also multiply ${W^{[1]}}^\top$ instead of $W^{[1]}$ in the matrix multiplication layer.  Such a change of notation is mostly for some consistence with the convention in the code.} We also have biases $b^{[1]} \in \mathbb{R}^m$ and $b^{[2]} \in \mathbb{R}^k$. The parameters of the model $\theta$ is $(W^{[1]},W^{[2]},b^{[1]},b^{[2]})$. The forward propagation equations for a single input $x^{(i)}$ then are:

\begin{align*}
  a^{(i)} &= \sigma \left( {W^{[1]}}^\top x^{(i)}  + b^{[1]} \right)  \in \mathbb{R}^m \\
  \bar{h}_{\theta}(x^{(i)})&= {W^{[2]}}^\top a^{(i)} + b^{[2]} \in \mathbb{R}^k \\
  {h}_{\theta}(x^{(i)}) &=  \mathrm{softmax}(\bar{h}_{\theta}(x^{(i)})) \in \mathbb{R}^k
\end{align*}
where $\sigma$ is the sigmoid function. 

For $\nexp$ training examples, we average the cross entropy loss over the $\nexp$ examples.
  \begin{equation*}
  J(W^{[1]},W^{[2]},b^{[1]},b^{[2]}) = \frac{1}{\nexp}\sum_{i=1}^\nexp \ell_\textup{CE}(\bar{h}_{\theta}(x^{(i)}),y^{(i)})  = - \frac{1}{\nexp}\sum_{i=1}^\nexp \log\left(\frac{\exp(\bar{h}_{\theta}(x^{(i)})_{y^{(i)}})}{\sum_{s=1}^{k}\exp({\bar{h}_{\theta}(x^{(i)})}_s)}\right).
  \end{equation*}

Suppose $e_y\in \Re^k$ is the one-hot embedding/representation of the discrete label $y$, where the $y$-th entry is 1 and all other entries are zeros. We can also write the loss function in the following way:
  \begin{equation*}
  J(W^{[1]},W^{[2]},b^{[1]},b^{[2]}) = - \frac{1}{\nexp}\sum_{i=1}^\nexp e_{\ysi}^\top\log\left(h_\theta(x^{(i)})\right).
  \end{equation*}
Here $\log(\cdot)$ is applied entry-wise to the vector $h_\theta(\xsi)$. The starter code already converts labels into one-hot representations for you.


Instead of batch gradient descent or stochastic gradient descent, the common practice
is to use mini-batch gradient descent for deep learning tasks. Concretely, we randomly sample $B$ examples $(x^{(i_b)}, y^{(i_b)})_{b=1}^B$ from $(x^{(i)}, y^{(i)})_{i=1}^n$. In this case, the
mini-batch cost function with batch-size $B$ is defined as follows:

  \begin{equation*}
  J_{MB} = \frac{1}{B}\sum_{b=1}^B \ell_\textup{CE}(\bar{h}_{\theta}(x^{(i_b)}),y^{(i_b)})
  \end{equation*}
where $B$ is the batch size, i.e., the number of training examples in each mini-batch. %\tnote{changed the indexing system here, please double check}

\begin{enumerate}
  \input{mnist/01-grad}

\ifnum\solutions=1 {
  \input{mnist/01-grad-sol}
} \fi

  \input{mnist/02-unregularized}

\ifnum\solutions=1 {
  \input{mnist/02-unregularized-sol}
} \fi

  \input{mnist/03-regularized}

\ifnum\solutions=1 {
  \input{mnist/03-regularized-sol}
} \fi


  \input{mnist/04-compare}
\ifnum\solutions=1 {
  \input{mnist/04-compare-sol}
} \fi

 \end{enumerate}

